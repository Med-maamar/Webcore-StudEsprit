# üìÑ G√©n√©ration Automatique de R√©sum√©s

## Vue d'ensemble

Le syst√®me g√©n√®re automatiquement des r√©sum√©s des cours √† partir de leurs PDFs en utilisant une technique d'**extraction de phrases cl√©s** (extractive summarization). Cette approche s√©lectionne les phrases les plus importantes du document original.

---

## üéØ Fonctionnalit√©s

### 1. G√©n√©ration de R√©sum√©
- **Analyse du PDF**: Extraction du texte complet
- **Identification des mots-cl√©s**: D√©tection des termes les plus fr√©quents
- **Scoring des phrases**: √âvaluation de l'importance de chaque phrase
- **S√©lection intelligente**: Choix des N phrases les plus repr√©sentatives
- **Ordre coh√©rent**: Maintien de l'ordre original pour la lisibilit√©

### 2. Statistiques Automatiques
- **Nombre de mots**: Comptage total dans le document
- **Nombre de phrases**: Phrases d√©tect√©es dans le document
- **Longueur du r√©sum√©**: Nombre de phrases s√©lectionn√©es
- **Sujets principaux**: Top 5 des mots-cl√©s extraits

### 3. Interface Utilisateur
- **Modal √©l√©gant**: Affichage dans une fen√™tre modale responsive
- **Cartes statistiques**: Visualisation des m√©triques
- **Tags de sujets**: Affichage des mots-cl√©s principaux
- **Bouton d'impression**: Export facile du r√©sum√©

---

## üî¨ Algorithme de G√©n√©ration

### √âtape 1: Extraction du Texte
```python
text = extract_text_from_pdf(pdf_path)
```
- Lecture du PDF avec PyPDF2
- Extraction page par page
- Concat√©nation du texte

### √âtape 2: Tokenisation
```python
sentences = sent_tokenize(text)
words = word_tokenize(text)
```
- D√©coupage en phrases avec NLTK
- D√©coupage en mots pour l'analyse

### √âtape 3: Filtrage des Mots
```python
filtered_words = [w for w in words 
                  if w not in stopwords 
                  and len(w) > 3]
```
- Suppression des mots vides (stopwords)
- Suppression des mots trop courts
- Focus sur les termes significatifs

### √âtape 4: Calcul des Fr√©quences
```python
word_freq = Counter(filtered_words)
top_keywords = word_freq.most_common(10)
```
- Comptage de la fr√©quence de chaque mot
- S√©lection des 10 mots les plus fr√©quents

### √âtape 5: Scoring des Phrases
```python
for sentence in sentences:
    score = sum(word_freq[word] for word in sentence)
    normalized_score = score / len(sentence_words)
```
- Score bas√© sur la fr√©quence des mots
- Normalisation par la longueur de la phrase
- √âvite le biais vers les phrases longues

### √âtape 6: S√©lection des Phrases
```python
top_sentences = sorted(scored_sentences)[:num_sentences]
top_sentences = sorted(top_sentences, key=original_order)
```
- S√©lection des N meilleures phrases
- Tri par ordre d'apparition original
- Maintien de la coh√©rence narrative

---

## üìä Structure de Sortie

```python
{
    'summary': 'This is a sample course document about Django development. Students will learn how to build web applications. The course covers models, views, templates.',
    'word_count': 99,
    'sentence_count': 10,
    'key_topics': ['django', 'course', 'students', 'development', 'framework'],
    'summary_length': 3
}
```

### Champs du R√©sum√©

| Champ | Type | Description |
|-------|------|-------------|
| `summary` | string | Texte du r√©sum√© (phrases s√©lectionn√©es) |
| `word_count` | int | Nombre total de mots dans le document |
| `sentence_count` | int | Nombre total de phrases dans le document |
| `key_topics` | list | Top 5 des mots-cl√©s les plus fr√©quents |
| `summary_length` | int | Nombre de phrases dans le r√©sum√© |

---

## üé® Interface Utilisateur

### Boutons dans le Tableau des Cours
```html
<!-- G√©n√©rer un nouveau r√©sum√© -->
<button hx-post="/program/cours/generate_summary/{{ cour_id }}/">
  üìÑ G√©n√©rer r√©sum√©
</button>

<!-- Voir un r√©sum√© existant -->
<button hx-get="/program/cours/view_summary/{{ cour_id }}/">
  üëÅÔ∏è Voir r√©sum√©
</button>
```

### Modal de R√©sum√©
- **En-t√™te**: Titre avec ic√¥ne
- **Info cours**: Nom du cours
- **Statistiques**: 3 cartes (mots, phrases, r√©sum√©)
- **Sujets**: Tags color√©s pour les mots-cl√©s
- **R√©sum√©**: Texte justifi√© dans un cadre
- **Info**: Note explicative sur la g√©n√©ration
- **Actions**: Boutons Imprimer et Fermer

---

## üîß Utilisation

### 1. Depuis l'Interface Web

1. **Aller sur la page des cours**
   ```
   /program/cours/
   ```

2. **Cliquer sur "G√©n√©rer r√©sum√©"**
   - Le syst√®me analyse le PDF
   - G√©n√®re le r√©sum√© automatiquement
   - Sauvegarde dans MongoDB
   - Affiche le modal avec le r√©sultat

3. **Consulter un r√©sum√© existant**
   - Cliquer sur "Voir r√©sum√©"
   - Affiche le r√©sum√© sauvegard√©

### 2. Depuis le Code Python

```python
from ml_service.generator import generate_summary_from_text

# G√©n√©rer un r√©sum√©
summary_data = generate_summary_from_text(
    pdf_path='/path/to/document.pdf',
    num_sentences=5  # Nombre de phrases souhait√©es
)

# Acc√©der aux donn√©es
print(f"R√©sum√©: {summary_data['summary']}")
print(f"Mots-cl√©s: {summary_data['key_topics']}")
print(f"Statistiques: {summary_data['word_count']} mots")
```

### 3. Depuis Django Views

```python
# Dans program/views.py
def cour_generate_summary(request, cid):
    # R√©cup√©rer le cours
    c = services.get_cour(cid)
    
    # G√©n√©rer le r√©sum√©
    summary_data = ml_generator.generate_summary_from_text(
        pdf_path, 
        num_sentences=5
    )
    
    # Sauvegarder
    services.update_cour(cid, {
        'generated_summary': summary_data
    })
    
    # Afficher
    return render(request, '_cours_summary_modal.html', {
        'summary_data': summary_data
    })
```

---

## üìà Param√®tres Personnalisables

### Longueur du R√©sum√©
```python
# R√©sum√© court (3 phrases)
generate_summary_from_text(pdf_path, num_sentences=3)

# R√©sum√© moyen (5 phrases)
generate_summary_from_text(pdf_path, num_sentences=5)

# R√©sum√© long (10 phrases)
generate_summary_from_text(pdf_path, num_sentences=10)
```

### Nombre de Mots-cl√©s
Modifiez dans `generator.py`:
```python
top_keywords = [w for w, _ in word_freq.most_common(10)]  # 10 mots
```

---

## ‚úÖ Tests Unitaires

### Test de G√©n√©ration
```python
def test_generate_summary_from_text(monkeypatch):
    # Mock PDF extraction
    monkeypatch.setattr(generator, 'extract_text_from_pdf', fake_extract)
    
    # Generate summary
    result = generator.generate_summary_from_text('/tmp/fake.pdf', num_sentences=3)
    
    # Verify structure
    assert 'summary' in result
    assert 'word_count' in result
    assert 'sentence_count' in result
    assert 'key_topics' in result
    assert result['summary_length'] == 3
```

### Ex√©cuter les Tests
```bash
pytest -v ml_service/tests/test_summary.py
```

**R√©sultat attendu:**
```
‚úì Summary generated successfully:
  - Word count: 99
  - Sentence count: 10
  - Summary length: 3 sentences
  - Key topics: ['course', 'django', 'students', 'development', 'framework']
```

---

## üéì Avantages et Limites

### ‚úÖ Avantages

1. **Rapide**: Traitement en quelques secondes
2. **Pas de hallucination**: Phrases r√©elles du document
3. **Fid√®le au contenu**: Pas d'interpr√©tation ou modification
4. **Multilingue**: Fonctionne avec n'importe quelle langue
5. **L√©ger**: Ne n√©cessite pas de mod√®le lourd (pas de GPT)

### ‚ö†Ô∏è Limites

1. **Extraction simple**: Pas de reformulation
2. **Coh√©rence limit√©e**: Phrases peuvent manquer de transition
3. **Pas de synth√®se**: Ne cr√©e pas de nouvelles phrases
4. **D√©pendant de la qualit√©**: Si le PDF contient peu de texte, r√©sum√© limit√©
5. **Pas d'analyse s√©mantique**: Bas√© uniquement sur la fr√©quence

---

## üöÄ Am√©liorations Futures

### 1. R√©sum√© Abstractif avec IA
```python
# Utiliser un mod√®le GPT ou BART pour reformuler
from transformers import pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
abstract_summary = summarizer(text, max_length=130, min_length=30)
```

### 2. Support Multilingue Am√©lior√©
- D√©tection automatique de la langue
- Stopwords adapt√©s √† chaque langue
- Tokenisation sp√©cifique

### 3. R√©sum√© Structur√©
- Extraction des titres et sections
- G√©n√©ration d'un plan
- R√©sum√© hi√©rarchique

### 4. Visualisations
- Nuage de mots-cl√©s
- Graphique de fr√©quence
- Carte conceptuelle

### 5. Export Avanc√©
- Export PDF stylis√©
- Export Word/DOCX
- Export Markdown

---

## üìö Technologies Utilis√©es

- **PyPDF2**: Extraction du texte des PDFs
- **NLTK**: Tokenisation et stopwords
- **Python Collections.Counter**: Comptage de fr√©quences
- **Django**: Framework web et gestion des vues
- **HTMX**: Chargement dynamique des modals
- **Tailwind CSS**: Styling moderne et responsive

---

## üí° Cas d'Usage

### Pour les √âtudiants
- Obtenir rapidement une vue d'ensemble d'un cours
- R√©viser les points cl√©s avant un examen
- Cr√©er des fiches de r√©vision

### Pour les Enseignants
- V√©rifier la coh√©rence du contenu
- Identifier les concepts principaux couverts
- Cr√©er des descriptions de cours

### Pour l'Administration
- Cataloguer les cours
- Cr√©er des r√©sum√©s pour les brochures
- Analyser le contenu p√©dagogique

---

## üîó Ressources

- [Documentation NLTK](https://www.nltk.org/)
- [PyPDF2 Documentation](https://pypdf2.readthedocs.io/)
- [Extractive Summarization](https://en.wikipedia.org/wiki/Automatic_summarization#Extractive_summarization)
- [Text Mining with Python](https://www.nltk.org/book/)

---

## üìû Support

Pour toute question ou am√©lioration, consultez:
- La documentation du projet
- Les tests unitaires dans `ml_service/tests/`
- Le code source dans `ml_service/generator.py`
